#!/usr/bin/env python3
"""
ä½¿ç”¨æ‰©å±•åçš„BERTåˆ†è¯å™¨ç¤ºä¾‹
å¯¹æ¯”åŸå§‹BERTåˆ†è¯å™¨å’Œæ‰©å±•ååˆ†è¯å™¨åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬ä¸Šçš„è¡¨ç°
"""

from transformers import AutoTokenizer
import os

def load_tokenizers():
    """
    åŠ è½½åŸå§‹BERTåˆ†è¯å™¨å’Œæ‰©å±•åçš„åˆ†è¯å™¨
    
    Returns:
        tuple: (åŸå§‹BERTåˆ†è¯å™¨, æ‰©å±•åçš„åˆ†è¯å™¨)
    """
    print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
    
    # åŠ è½½åŸå§‹BERTåˆ†è¯å™¨
    try:
        original_bert = AutoTokenizer.from_pretrained('bert-base-uncased')
        print(f"âœ… åŸå§‹BERTåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {len(original_bert.vocab)}")
    except Exception as e:
        print(f"âŒ åŸå§‹BERTåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    # åŠ è½½æ‰©å±•åçš„åˆ†è¯å™¨
    extended_path = "extended_bert_output"
    if os.path.exists(extended_path):
        try:
            extended_bert = AutoTokenizer.from_pretrained(extended_path)
            print(f"âœ… æ‰©å±•åBERTåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼Œè¯æ±‡è¡¨å¤§å°: {len(extended_bert.vocab)}")
        except Exception as e:
            print(f"âŒ æ‰©å±•åBERTåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            return original_bert, None
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æ‰©å±•åçš„åˆ†è¯å™¨ç›®å½•: {extended_path}")
        return original_bert, None
    
    return original_bert, extended_bert

def compare_tokenization(original_tokenizer, extended_tokenizer, texts):
    """
    å¯¹æ¯”ä¸¤ä¸ªåˆ†è¯å™¨çš„åˆ†è¯æ•ˆæœ
    
    Args:
        original_tokenizer: åŸå§‹åˆ†è¯å™¨
        extended_tokenizer: æ‰©å±•åçš„åˆ†è¯å™¨
        texts (list): æµ‹è¯•æ–‡æœ¬åˆ—è¡¨
    """
    print("\n" + "="*80)
    print("åˆ†è¯æ•ˆæœè¯¦ç»†å¯¹æ¯”")
    print("="*80)
    
    total_original_tokens = 0
    total_extended_tokens = 0
    improved_cases = 0
    
    for i, text in enumerate(texts, 1):
        print(f"\nã€æµ‹è¯• {i}ã€‘")
        print(f"åŸæ–‡: {text}")
        print("-" * 60)
        
        # åŸå§‹BERTåˆ†è¯
        original_tokens = original_tokenizer.tokenize(text)
        original_ids = original_tokenizer.encode(text, add_special_tokens=True)
        
        # æ‰©å±•BERTåˆ†è¯
        extended_tokens = extended_tokenizer.tokenize(text)
        extended_ids = extended_tokenizer.encode(text, add_special_tokens=True)
        
        print(f"åŸå§‹BERT ({len(original_tokens)} tokens):")
        print(f"  åˆ†è¯: {original_tokens}")
        print(f"  Token IDs: {original_ids}")
        
        print(f"æ‰©å±•BERT ({len(extended_tokens)} tokens):")
        print(f"  åˆ†è¯: {extended_tokens}")
        print(f"  Token IDs: {extended_ids}")
        
        # åˆ†ææ”¹è¿›
        token_diff = len(original_tokens) - len(extended_tokens)
        if token_diff > 0:
            print(f"âœ… æ”¹è¿›: å‡å°‘äº† {token_diff} ä¸ªtoken ({token_diff/len(original_tokens)*100:.1f}%)")
            improved_cases += 1
        elif token_diff == 0:
            print("â– æ— å˜åŒ–")
        else:
            print(f"âŒ å¢åŠ äº† {-token_diff} ä¸ªtoken")
        
        total_original_tokens += len(original_tokens)
        total_extended_tokens += len(extended_tokens)
    
    print("\n" + "="*80)
    print("æ€»ä½“ç»Ÿè®¡")
    print("="*80)
    print(f"æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(texts)}")
    print(f"æ”¹è¿›æ¡ˆä¾‹æ•°é‡: {improved_cases}")
    print(f"åŸå§‹BERTæ€»tokenæ•°: {total_original_tokens}")
    print(f"æ‰©å±•BERTæ€»tokenæ•°: {total_extended_tokens}")
    
    reduction_rate = (total_original_tokens - total_extended_tokens) / total_original_tokens * 100
    print(f"Tokenå‡å°‘ç‡: {reduction_rate:.2f}%")
    
    avg_original = total_original_tokens / len(texts)
    avg_extended = total_extended_tokens / len(texts)
    print(f"å¹³å‡tokenæ•° - åŸå§‹: {avg_original:.1f}, æ‰©å±•: {avg_extended:.1f}")

def analyze_specific_terms(original_tokenizer, extended_tokenizer):
    """
    åˆ†æç‰¹å®šç”Ÿç‰©åŒ»å­¦æœ¯è¯­çš„åˆ†è¯æ”¹è¿›
    
    Args:
        original_tokenizer: åŸå§‹åˆ†è¯å™¨
        extended_tokenizer: æ‰©å±•åçš„åˆ†è¯å™¨
    """
    print("\n" + "="*80)
    print("ç”Ÿç‰©åŒ»å­¦æœ¯è¯­åˆ†è¯å¯¹æ¯”")
    print("="*80)
    
    biomedical_terms = [
        "immunotherapy",
        "myocardial infarction", 
        "mitochondrial dysfunction",
        "transcriptional regulation",
        "protein expression",
        "cellular respiration",
        "antibiotics resistance",
        "gene editing",
        "neurodegenerative diseases",
        "cardiovascular risk",
        "pharmacokinetics",
        "biomarkers",
        "pathogenesis",
        "therapeutic targets",
        "clinical trials",
        "molecular mechanisms",
        "epigenetic modifications",
        "stem cell differentiation",
        "metabolic pathways",
        "inflammatory response"
    ]
    
    significant_improvements = []
    
    for term in biomedical_terms:
        original_tokens = original_tokenizer.tokenize(term)
        extended_tokens = extended_tokenizer.tokenize(term)
        
        token_reduction = len(original_tokens) - len(extended_tokens)
        reduction_percent = (token_reduction / len(original_tokens)) * 100 if len(original_tokens) > 0 else 0
        
        print(f"\næœ¯è¯­: {term}")
        print(f"  åŸå§‹: {original_tokens} ({len(original_tokens)} tokens)")
        print(f"  æ‰©å±•: {extended_tokens} ({len(extended_tokens)} tokens)")
        
        if token_reduction > 0:
            print(f"  âœ… å‡å°‘ {token_reduction} ä¸ªtoken ({reduction_percent:.1f}%)")
            if reduction_percent >= 25:  # æ˜¾è‘—æ”¹è¿›
                significant_improvements.append((term, reduction_percent))
        elif token_reduction == 0:
            print(f"  â– æ— å˜åŒ–")
        else:
            print(f"  âŒ å¢åŠ  {-token_reduction} ä¸ªtoken")
    
    if significant_improvements:
        print(f"\næ˜¾è‘—æ”¹è¿›çš„æœ¯è¯­ (å‡å°‘â‰¥25%):")
        for term, improvement in sorted(significant_improvements, key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {term}: {improvement:.1f}% æ”¹è¿›")

def demonstrate_use_cases(extended_tokenizer):
    """
    æ¼”ç¤ºæ‰©å±•ååˆ†è¯å™¨çš„å®é™…ä½¿ç”¨åœºæ™¯
    
    Args:
        extended_tokenizer: æ‰©å±•åçš„åˆ†è¯å™¨
    """
    print("\n" + "="*80)
    print("å®é™…ä½¿ç”¨åœºæ™¯æ¼”ç¤º")
    print("="*80)
    
    use_cases = [
        {
            "scenario": "åŒ»å­¦æŠ¥å‘Šæ‘˜è¦",
            "text": "Patient presents with acute myocardial infarction. Elevated troponin levels and ST-segment elevation observed. Emergency percutaneous coronary intervention performed successfully."
        },
        {
            "scenario": "ç”Ÿç‰©å­¦ç ”ç©¶è®ºæ–‡",
            "text": "The study investigated transcriptional regulation of inflammatory genes in macrophages. RNA sequencing revealed differential expression patterns following cytokine stimulation."
        },
        {
            "scenario": "è¯ç‰©ç ”å‘æ–‡æ¡£",
            "text": "The novel immunotherapy demonstrates promising efficacy against metastatic cancer cells. Phase II clinical trials showed significant tumor reduction with minimal adverse effects."
        },
        {
            "scenario": "åŸºå› ç»„å­¦ç ”ç©¶",
            "text": "CRISPR-Cas9 gene editing enables precise modification of genomic sequences. The technology shows potential for treating inherited genetic disorders."
        }
    ]
    
    for case in use_cases:
        print(f"\nåœºæ™¯: {case['scenario']}")
        print(f"æ–‡æœ¬: {case['text']}")
        
        tokens = extended_tokenizer.tokenize(case['text'])
        encoded = extended_tokenizer.encode(case['text'], add_special_tokens=True, 
                                          max_length=512, truncation=True)
        
        print(f"åˆ†è¯ç»“æœ ({len(tokens)} tokens):")
        print(f"  {tokens}")
        print(f"ç¼–ç é•¿åº¦: {len(encoded)}")
        print(f"å¯ç”¨äºæ¨¡å‹è¾“å…¥: {'âœ…' if len(encoded) <= 512 else 'âŒ è¶…å‡ºé•¿åº¦é™åˆ¶'}")

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("æ‰©å±•BERTåˆ†è¯å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("å¯¹æ¯”åŸå§‹BERTåˆ†è¯å™¨å’Œæ‰©å±•ååˆ†è¯å™¨åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬ä¸Šçš„è¡¨ç°")
    print("="*80)
    
    # åŠ è½½åˆ†è¯å™¨
    original_tokenizer, extended_tokenizer = load_tokenizers()
    
    if original_tokenizer is None or extended_tokenizer is None:
        print("âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "The patient was diagnosed with acute myocardial infarction following coronary artery occlusion.",
        "SARS-CoV-2 viral RNA was detected using reverse transcription polymerase chain reaction.",
        "Immunotherapy with checkpoint inhibitors has shown remarkable efficacy in cancer treatment.",
        "The protein expression levels were analyzed using Western blot and immunofluorescence microscopy.",
        "Gene editing with CRISPR-Cas9 technology enables precise modification of DNA sequences.",
        "Mitochondrial dysfunction leads to impaired cellular respiration and energy metabolism.",
        "Antibiotics resistance mechanisms include enzymatic degradation and efflux pump activation.",
        "Stem cell differentiation is regulated by transcription factors and epigenetic modifications."
    ]
    
    # 1. å¯¹æ¯”åˆ†è¯æ•ˆæœ
    compare_tokenization(original_tokenizer, extended_tokenizer, test_texts)
    
    # 2. åˆ†æç‰¹å®šæœ¯è¯­
    analyze_specific_terms(original_tokenizer, extended_tokenizer)
    
    # 3. å®é™…ä½¿ç”¨åœºæ™¯æ¼”ç¤º
    demonstrate_use_cases(extended_tokenizer)
    
    print("\n" + "="*80)
    print("æ€»ç»“")
    print("="*80)
    print("âœ… æ‰©å±•åçš„BERTåˆ†è¯å™¨åœ¨ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬å¤„ç†æ–¹é¢è¡¨ç°å‡ºä»¥ä¸‹ä¼˜åŠ¿:")
    print("   â€¢ æ›´å¥½åœ°è¯†åˆ«å®Œæ•´çš„åŒ»å­¦æœ¯è¯­")
    print("   â€¢ å‡å°‘äº†å¤æ‚æœ¯è¯­çš„å­è¯åˆ†å‰²")
    print("   â€¢ æé«˜äº†åˆ†è¯çš„è¯­ä¹‰è¿è´¯æ€§")
    print("   â€¢ å‡å°‘äº†å¹³å‡tokenæ•°é‡ï¼Œæé«˜äº†æ¨¡å‹æ•ˆç‡")
    print("\nğŸ“ æ‰©å±•åçš„åˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨: extended_bert_output/")
    print("ğŸ’¡ å¯ä»¥ç›´æ¥ä½¿ç”¨ AutoTokenizer.from_pretrained('extended_bert_output') åŠ è½½")

if __name__ == "__main__":
    main() 